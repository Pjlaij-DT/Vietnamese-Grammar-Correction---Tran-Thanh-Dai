{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "13W43vP2P8P5",
        "outputId": "b6a51efe-c648-422b-de2a-bb680b4fc08f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: underthesea in /usr/local/lib/python3.10/dist-packages (6.8.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.10/dist-packages (from underthesea) (8.1.7)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.6 in /usr/local/lib/python3.10/dist-packages (from underthesea) (0.9.10)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from underthesea) (3.8.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from underthesea) (4.66.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from underthesea) (2.31.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.4.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.2.2)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from underthesea) (6.0.1)\n",
            "Requirement already satisfied: underthesea-core==1.0.4 in /usr/local/lib/python3.10/dist-packages (from underthesea) (1.0.4)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->underthesea) (2023.12.25)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->underthesea) (2024.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (1.11.4)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->underthesea) (3.5.0)\n",
            "Requirement already satisfied: Unidecode in /usr/local/lib/python3.10/dist-packages (1.3.8)\n"
          ]
        }
      ],
      "source": [
        "!pip install underthesea\n",
        "!pip install Unidecode"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "__WCx43wP_23",
        "outputId": "283da4e3-49f7-4f03-d691-a43c51282833"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-66-d5df0069828e>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    131\u001b[0m   )\n\u001b[1;32m    132\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "U4UoZ1hVQ3vu"
      },
      "outputs": [],
      "source": [
        "!unzip \"/content/drive/MyDrive/DL_Final/dataset.zip\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb-sGj7cVMzU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from underthesea import sent_tokenize\n",
        "from collections import Counter\n",
        "from underthesea import word_tokenize\n",
        "from torch.utils.data import Dataset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import numpy as np\n",
        "import unidecode\n",
        "import re\n",
        "import random\n",
        "import torch.nn.functional as F\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pH-piSuIVNVd"
      },
      "outputs": [],
      "source": [
        "data_path = \"corpus-title.txt\"\n",
        "import pickle\n",
        "# Save vocabulary to a file\n",
        "def save_vocab(vocab, file_path):\n",
        "    with open(file_path, 'wb') as file:\n",
        "        pickle.dump(vocab, file)\n",
        "\n",
        "# Load vocabulary from a file\n",
        "def load_vocab(file_path):\n",
        "    with open(file_path, 'rb') as file:\n",
        "        vocab = pickle.load(file)\n",
        "    return vocab\n",
        "vocab_file_path = \"vocab.pkl\"\n",
        "checkpoint_file_path = \"/content/drive/MyDrive/DL_Final/seq2seq_checkpoint_epoch_5.pt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tdu4fivfY24N"
      },
      "outputs": [],
      "source": [
        "class Vocabulary:\n",
        "    def __init__(self,freq_threshold):\n",
        "        #setting the pre-reserved tokens int to string tokens\n",
        "        self.itos = {0:\"<PAD>\",1:\"<SOS>\",2:\"<EOS>\",3:\"<UNK>\"}\n",
        "\n",
        "        #string to int tokens\n",
        "        #its reverse dict self.itos\n",
        "        self.stoi = {v:k for k,v in self.itos.items()}\n",
        "\n",
        "        self.freq_threshold = freq_threshold\n",
        "\n",
        "    def __len__(self): return len(self.itos)\n",
        "\n",
        "    @staticmethod\n",
        "    def tokenize(text):\n",
        "        return [token.lower() for token in word_tokenize(text)]\n",
        "\n",
        "    def build_vocab(self, sentence_list):\n",
        "        frequencies = Counter()\n",
        "        idx = 4\n",
        "        for sentence in tqdm(sentence_list):\n",
        "            for word in self.tokenize(sentence):\n",
        "                frequencies[word] += 1\n",
        "                #add the word to the vocab if it reaches minum frequecy threshold\n",
        "                if frequencies[word] == self.freq_threshold:\n",
        "                    self.stoi[word] = idx\n",
        "                    self.itos[idx] = word\n",
        "                    idx += 1\n",
        "\n",
        "    def numericalize(self,text):\n",
        "        tokenized_text = self.tokenize(text)\n",
        "        return [ self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text ]\n",
        "\n",
        "    def get_pad_index(self):\n",
        "        return self.stoi[\"<PAD>\"]\n",
        "\n",
        "    def get_start_index(self):\n",
        "        return self.stoi[\"<SOS>\"]\n",
        "\n",
        "    def get_end_index(self):\n",
        "        return self.stoi[\"<EOS>\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J35ppS60rnxq"
      },
      "outputs": [],
      "source": [
        "accented_chars_vietnamese = [\n",
        "    'á', 'à', 'ả', 'ã', 'ạ', 'â', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ă', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ',\n",
        "    'ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ', 'ớ', 'ờ', 'ở', 'ỡ', 'ợ',\n",
        "    'é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ê', 'ế', 'ề', 'ể', 'ễ', 'ệ',\n",
        "    'ú', 'ù', 'ủ', 'ũ', 'ụ', 'ư', 'ứ', 'ừ', 'ử', 'ữ', 'ự',\n",
        "    'í', 'ì', 'ỉ', 'ĩ', 'ị',\n",
        "    'ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ',\n",
        "    'đ',\n",
        "]\n",
        "\n",
        "accented_chars_vietnamese.extend([c.upper() for c in accented_chars_vietnamese])\n",
        "alphabet = list(\"aAàÀảẢãÃáÁạẠăĂằẰẳẲẵẴắẮặẶâÂầẦẩẨẫẪấẤậẬbBcCdDđĐeEèÈẻẺẽẼéÉẹẸêÊềỀểỂễỄếẾệỆfFgGhHiIìÌỉỈĩĨíÍịỊjJkKlLmMnNoOòÒỏỎõÕóÓọỌôÔồỒổỔỗỖốỐộỘơƠờỜởỞỡỠớỚợỢpPqQrRsStTuUùÙủỦũŨúÚụỤưƯừỪửỬữỮứỨựỰvVwWxXyYỳỲỷỶỹỸýÝỵỴzZ0123456789! \\\"#$%&''()*+,-./:;<=>?@[\\]^_{|}~\")\n",
        "\n",
        "chars_regrex = '[aàảãáạăằẳẵắặâầẩẫấậAÀẢÃÁẠĂẰẲẴẮẶÂẦẨẪẤẬoòỏõóọôồổỗốộơờởỡớợOÒỎÕÓỌÔỒỔỖỐỘƠỜỞỠỚỢeèẻẽéẹêềểễếệEÈẺẼÉẸÊỀỂỄẾỆuùủũúụưừửữứựUÙỦŨÚỤƯỪỬỮỨỰiìỉĩíịIÌỈĨÍỊyỳỷỹýỵYỲỶỸÝỴnNvVmMCG]'\n",
        "same_chars = {\n",
        "    'a': ['á', 'à', 'ả', 'ã', 'ạ', 'ấ', 'ầ', 'ẩ', 'ẫ', 'ậ', 'ắ', 'ằ', 'ẳ', 'ẵ', 'ặ'],\n",
        "    'A': ['Á','À','Ả','Ã','Ạ','Ấ','Ầ','Ẩ','Ẫ','Ậ','Ắ','Ằ','Ẳ','Ẵ','Ặ'],\n",
        "    'O': ['Ó','Ò','Ỏ','Õ','Ọ','Ô','Ố','Ồ','Ổ','Ỗ','Ộ','Ơ','Ớ','Ờ','Ở','Ỡ','Ợ','Q'],\n",
        "    'o': ['ó', 'ò', 'ỏ', 'õ', 'ọ', 'ô', 'ố', 'ồ', 'ổ', 'ỗ', 'ộ', 'ơ','ớ', 'ờ', 'ở', 'ỡ', 'ợ', 'q'],\n",
        "    'e': ['é', 'è', 'ẻ', 'ẽ', 'ẹ', 'ế', 'ề', 'ể', 'ễ', 'ệ', 'ê'],\n",
        "    'E': ['É', 'È', 'Ẻ', 'Ẽ', 'Ẹ', 'Ế', 'Ề', 'Ể', 'Ễ', 'Ệ', 'Ê'],\n",
        "    'u': ['ú', 'ù', 'ủ', 'ũ', 'ụ', 'ứ', 'ừ', 'ử', 'ữ', 'ự', 'ư'],\n",
        "    'U': ['Ú', 'Ù', 'Ủ', 'Ũ', 'Ụ', 'Ứ', 'Ừ', 'Ử', 'Ữ', 'Ự', 'Ư'],\n",
        "    'i': ['í', 'ì', 'ỉ', 'ĩ', 'ị'],\n",
        "    'I': ['Í', 'Ì', 'Ỉ', 'Ĩ', 'Ị'],\n",
        "    'y': ['ý', 'ỳ', 'ỷ', 'ỹ', 'ỵ', 'v'],\n",
        "    'Y': ['Ý', 'Ỳ', 'Ỷ', 'Ỹ', 'Ỵ', 'V'],\n",
        "    'n': ['m'],\n",
        "    'N': ['N'],\n",
        "    'v': ['y'],\n",
        "    'V': ['Y'],\n",
        "    'm': ['n'],\n",
        "    'M': ['N'],\n",
        "    'C': ['G'],\n",
        "    'G': ['C']\n",
        "}\n",
        "\n",
        "indices = [alphabet.index(c) + 4 for c in accented_chars_vietnamese]\n",
        "vocab_size = len(alphabet)\n",
        "litle_char = list('\"#$%&''()*+,-./:;<=>?@[\\]^_{|}~')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bNuyU8-3soU4"
      },
      "outputs": [],
      "source": [
        "def remove_random_accent(text, ratio=0.15):\n",
        "        text = ''.join(text)\n",
        "        words = text.split()\n",
        "        mask = np.random.random(size=len(words)) < ratio\n",
        "\n",
        "        for i in range(len(words)):\n",
        "            if mask[i]:\n",
        "                words[i] = unidecode.unidecode(words[i])\n",
        "                break\n",
        "\n",
        "        return ' '.join(words)\n",
        "\n",
        "def remove_random_space(text):\n",
        "        text = ''.join(text)\n",
        "        words = text.split()\n",
        "        n_words = len(words)\n",
        "        start = np.random.randint(low=0, high=n_words, size=1)[0]\n",
        "\n",
        "        if start + 3 < n_words:\n",
        "            end = np.random.randint(low=start, high=start + 3, size=1)[0]\n",
        "        else:\n",
        "            end = np.random.randint(low=start, high=n_words, size=1)[0]\n",
        "\n",
        "        out = ' '.join(words[:start])  + ' ' + ''.join(words[start:end + 1]) + ' ' + ' '.join(words[end + 1:])\n",
        "\n",
        "        return out.strip()\n",
        "\n",
        "def _char_regrex(text):\n",
        "        match_chars = re.findall(chars_regrex, text)\n",
        "\n",
        "        return match_chars\n",
        "\n",
        "def _random_replace(text, match_chars):\n",
        "        replace_char = match_chars[np.random.randint(low=0, high=len(match_chars), size=1)[0]]\n",
        "        insert_chars = same_chars[unidecode.unidecode(replace_char)]\n",
        "        insert_char = insert_chars[np.random.randint(low=0, high=len(insert_chars), size=1)[0]]\n",
        "        text = text.replace(replace_char, insert_char, 1)\n",
        "\n",
        "        return text\n",
        "\n",
        "def change(text):\n",
        "        match_chars = _char_regrex(text)\n",
        "        if len(match_chars) == 0:\n",
        "            return text\n",
        "        text = ''.join(text)\n",
        "        text = _random_replace(text, match_chars)\n",
        "\n",
        "        return text\n",
        "\n",
        "def replace_accent_chars(text, ratio=0.15):\n",
        "        text = ''.join(text)\n",
        "        words = text.split()\n",
        "        mask = np.random.random(size=len(words)) < ratio\n",
        "\n",
        "        for i in range(len(words)):\n",
        "            if mask[i]:\n",
        "                words[i] = change(words[i])\n",
        "                break\n",
        "\n",
        "        return ' '.join(words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K437UTTjW-T1"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, data_path, vocab):\n",
        "        self.data = self.load_data(data_path)\n",
        "        self.vocab = vocab\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "    def __getitem__(self, idx):\n",
        "      function_list = [remove_random_accent, replace_accent_chars,remove_random_space]\n",
        "      random_function = random.choice(function_list)\n",
        "      target_text = ''.join(self.data[idx])\n",
        "      source_text = random_function(source_text)\n",
        "      numericalized_source = self.vocab.numericalize(source_text)\n",
        "      numericalized_target = self.vocab.numericalize(target_text)\n",
        "\n",
        "      # Pad or truncate source and target sequences to a fixed length\n",
        "      max_length = 20  # Define your desired maximum sequence length\n",
        "      numericalized_source = self.pad_or_truncate(numericalized_source, max_length)\n",
        "      numericalized_target = self.pad_or_truncate(numericalized_target, max_length)\n",
        "      return torch.tensor(numericalized_source), torch.tensor(numericalized_target, dtype=torch.long)\n",
        "\n",
        "    def pad_or_truncate(self, sequence, max_length):\n",
        "      if len(sequence) < max_length:\n",
        "          # Pad sequence with padding token\n",
        "          padding_token = str(self.vocab.get_pad_index())\n",
        "          sequence += [padding_token] * (max_length - len(sequence))\n",
        "      elif len(sequence) > max_length:\n",
        "          # Truncate sequence to max_length\n",
        "          sequence = sequence[:max_length]\n",
        "      sequence = [ int(x) for x in sequence ]\n",
        "      return sequence\n",
        "\n",
        "    def load_data(self, data_path):\n",
        "        # Load data from the file\n",
        "        with open(data_path, 'r', encoding='utf-8') as file:\n",
        "            #lines = [line.strip().split('\\t') for line in file.readlines()]\n",
        "            lines = [file.readline().strip() for _ in range(200000)]\n",
        "            print(\"Reading dataset with size: \" + str(len(lines)))\n",
        "        return lines\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jXfv7eOWbcvk"
      },
      "outputs": [],
      "source": [
        "freq_threshold = 5\n",
        "if not Path(vocab_file_path).is_file():\n",
        "  vocab = Vocabulary(freq_threshold)\n",
        "  dataset = CustomDataset(data_path, vocab)\n",
        "  vocab.build_vocab(dataset.data)\n",
        "else:\n",
        "  vocab = Vocabulary(freq_threshold)\n",
        "  vocab = load_vocab(vocab_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdkcC88_jzni"
      },
      "outputs": [],
      "source": [
        "save_vocab(vocab, vocab_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zjj2TFSOmHDy"
      },
      "outputs": [],
      "source": [
        "def collate_batch(batch):\n",
        "    \"\"\"\n",
        "    Collate function for DataLoader to pad sequences.\n",
        "\n",
        "    Args:\n",
        "    batch (list): List of tuples, each containing a source sequence and a target sequence.\n",
        "\n",
        "    Returns:\n",
        "    tuple: A tuple containing padded source sequences and padded target sequences.\n",
        "    \"\"\"\n",
        "    # Unzip the batch into source and target sequences\n",
        "    src_batch, trg_batch = zip(*batch)\n",
        "\n",
        "    # Find the lengths of each source and target sequence\n",
        "    src_lengths = [len(seq) for seq in src_batch]\n",
        "    trg_lengths = [len(seq) for seq in trg_batch]\n",
        "\n",
        "    # Find the maximum length in each batch\n",
        "    max_src_length = max(src_lengths)\n",
        "    max_trg_length = max(trg_lengths)\n",
        "\n",
        "    # Get the padding index\n",
        "    pad_index = vocab.get_pad_index()\n",
        "\n",
        "    # Pad the source sequences\n",
        "    src_padded = torch.nn.utils.rnn.pad_sequence(src_batch, batch_first=True, padding_value=pad_index)\n",
        "\n",
        "    # Pad the target sequences\n",
        "    trg_padded = torch.nn.utils.rnn.pad_sequence(trg_batch, batch_first=True, padding_value=pad_index)\n",
        "\n",
        "    return src_padded, trg_padded\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W8bXOVz3k-Uj"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_dim, emb_dim, hidden_dim, num_layers):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers)\n",
        "\n",
        "    def forward(self, src):\n",
        "        embedded = self.embedding(src)\n",
        "        outputs, (hidden, cell) = self.rnn(embedded)\n",
        "        return hidden, cell\n",
        "\n",
        "# Define your Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_dim, emb_dim, hidden_dim, num_layers):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.embedding = nn.Embedding(output_dim, emb_dim)\n",
        "        self.rnn = nn.LSTM(emb_dim, hidden_dim, num_layers)\n",
        "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
        "\n",
        "    def forward(self, input, hidden, cell):\n",
        "        input = input.unsqueeze(0)\n",
        "        embedded = self.embedding(input)\n",
        "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
        "        prediction = self.fc_out(output.squeeze(0))\n",
        "        return prediction, hidden, cell\n",
        "\n",
        "# Define your Seq2Seq model\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, encoder, decoder):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "\n",
        "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
        "        batch_size = trg.shape[1]\n",
        "        max_len = trg.shape[0]\n",
        "        trg_vocab_size = self.decoder.fc_out.out_features\n",
        "\n",
        "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(src.device)\n",
        "        hidden, cell = self.encoder(src)\n",
        "\n",
        "        input = trg[0, :]\n",
        "\n",
        "        for t in range(1, max_len):\n",
        "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
        "            outputs[t] = output\n",
        "            teacher_force = torch.rand(1).item() < teacher_forcing_ratio\n",
        "            top1 = output.argmax(1)\n",
        "            input = trg[t] if teacher_force else top1\n",
        "\n",
        "        return outputs\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y178slAHk-6o"
      },
      "outputs": [],
      "source": [
        "# Define parameters\n",
        "input_dim = len(vocab)\n",
        "output_dim = len(vocab)\n",
        "emb_dim = 256\n",
        "hidden_dim = 512\n",
        "num_layers = 2\n",
        "batch_size = 64\n",
        "num_epochs = 5\n",
        "learning_rate = 0.001\n",
        "checkpoint_path = \"/content/drive/MyDrive/DL_Final/seq2seq_checkpoint_epoch_{}.pt\"\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Create an instance of the encoder and decoder\n",
        "encoder = Encoder(input_dim, emb_dim, hidden_dim, num_layers)\n",
        "decoder = Decoder(output_dim, emb_dim, hidden_dim, num_layers)\n",
        "\n",
        "# Create an instance of the Seq2Seq model\n",
        "model = Seq2Seq(encoder, decoder).to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Create DataLoader for training\n",
        "train_dataset = CustomDataset(data_path, vocab)\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
        "\n",
        "if not Path(checkpoint_file_path).is_file():\n",
        "  # Training loop\n",
        "  for epoch in range(num_epochs):\n",
        "      total_loss = 0\n",
        "      for src, trg in tqdm(train_loader):\n",
        "          src = src.to(device)\n",
        "          trg = trg.to(device)\n",
        "\n",
        "          optimizer.zero_grad()\n",
        "          output = model(src, trg)\n",
        "\n",
        "          # Shape of output: (trg_len, batch_size, output_dim)\n",
        "          # Shape of trg: (trg_len, batch_size)\n",
        "          output_dim = output.shape[-1]\n",
        "\n",
        "          output = output[1:].view(-1, output_dim)\n",
        "          trg = trg[1:].view(-1)\n",
        "\n",
        "          loss = criterion(output, trg)\n",
        "\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          total_loss += loss.item()\n",
        "\n",
        "      print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {total_loss / len(train_loader)}\")\n",
        "      avg_loss = total_loss / len(train_loader)\n",
        "      checkpoint_file = checkpoint_path.format(epoch + 1)\n",
        "      torch.save({\n",
        "          'epoch': epoch + 1,\n",
        "          'model_state_dict': model.state_dict(),\n",
        "          'optimizer_state_dict': optimizer.state_dict(),\n",
        "          'loss': avg_loss,\n",
        "          # Add any other relevant information\n",
        "      }, checkpoint_file)\n",
        "else:\n",
        "  checkpoint = torch.load(checkpoint_file_path, map_location=torch.device('cpu'))\n",
        "  model.load_state_dict(checkpoint['model_state_dict'])\n",
        "  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "  start_epoch = checkpoint['epoch']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3DGtVZEi6j9"
      },
      "outputs": [],
      "source": [
        "def translate_sentence(sentence, model, device, max_length=50, sos_token_idx=1, eos_token_idx=2):\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Tokenize the input sentence\n",
        "        numericalized_sentence = [vocab.numericalize(token) for token in sentence.split()]\n",
        "        numericalized_sentence = list(itertools.chain.from_iterable(numericalized_sentence))\n",
        "        numericalized_sentence = torch.tensor(numericalized_sentence).unsqueeze(1).to(device)  # Modified here\n",
        "       # numericalized_sentence = torch.tensor(numericalized_sentence).to(device)\n",
        "\n",
        "        # Forward pass through the model\n",
        "        hidden, cell = model.encoder(numericalized_sentence)\n",
        "        output_sequence = [sos_token_idx]\n",
        "\n",
        "        for _ in range(max_length):\n",
        "            input_tensor = torch.tensor([output_sequence[-1]]).to(device)\n",
        "            output, hidden, cell = model.decoder(input_tensor, hidden, cell)\n",
        "            output_token = output.argmax(1).item()\n",
        "            output_sequence.append(output_token)\n",
        "            if output_token == eos_token_idx:\n",
        "                break\n",
        "\n",
        "        # Convert output tokens to text\n",
        "        output_sentence = [vocab.itos[idx] for idx in output_sequence[1:-1]]  # Exclude <sos> and <eos> tokens\n",
        "        return ' '.join(output_sentence)\n",
        "\n",
        "# Example usage:\n",
        "input_sentence = \"Bạn có thấy khỏe không?\"\n",
        "output_sentence = translate_sentence(input_sentence, model, device)\n",
        "print(output_sentence)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}